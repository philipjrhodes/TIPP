- delaunay01: MPI shared file, number of tasks sending to MPI each time may be greater than number of cores, initial Delaunay is NOT improved as the one running in MPI (slaves)

- delaunay02: serial version, not running on MPI, initial Delaunay is improved as the one running in MPI (slaves)

- delaunay03: MPI shared file, number of tasks sending to MPI each time may be greater than number of cores, initial Delaunay is NOT improved as the one running in MPI (slaves)

- delaunay04: MPI NOT shared file --> meaning each slave node store it own file and the master will merge all file into one later, number of tasks sending to MPI each time may be greater than number of cores, initial Delaunay is NOT improved as the one running in MPI (slaves)

- delaunay06: MPI shared file, number of tasks sending to MPI each time equal to number of cores, initial Delaunay is NOT improved as the one running in MPI (slaves)

- delaunay07_Float: similar to delaunay06 but use float for coordinates, not test yet, something wrong

- delaunay08: MPI shared file, number of tasks sending to MPI each time equal to number of cores, initial Delaunay is improved as the one running in MPI (slaves)

- delaunay09: Similar to delaunay08, however, the indices of point corrdinates are not in order from partition 0 to partition N. The indices of points are still keep the same with the order of coordinates in file mydatabin.ver in folder rawPointData.
The file fullPointPart.ver = mydatabin.ver + gridPoints.
- initPoint.ver, pointCoorPart, pointIdPart.ver, and pointPartInfo.xfdl are not needed after generating fullPointPart.ver and triangleIds.tri
- files that are changed compared to delaunay08: point.h, point.cpp, distribute.h, distribute.cpp, distributeMain.cpp, domain.h, domain.cpp, doaminMain.cpp, delaunayMPI.cpp
- Design for very large dataset, the domain may be larger (0,0 - 1,1)
- The graph is added more functions: drawReturnAndStoreTriangles()

- delaunay10: delaunayMPI.cpp is built in class.
	+ MPI runs separately from the main code
	+ problem: read share large file (pointCoorPart.ver, pointIdPart.ver), overflow some of the parameters

- delaunay11: bad version
	+ can use PBS, run directly on MPI by delaunayMPI.cpp
	+ problem: read share large file (pointCoorPart.ver, pointIdPart.ver), overflow some of the parameters
	+ write back to master use parallel write (MPI_write)
	+ drawDomain.cpp draw whole domain after triangulation

- delaunay12:
	+ good version:

- delaunay13: bad version
- delaunay15: testing with multiple masters

Delaunay0 used for 1000 vertices (1Kvertices), with graphic
Delaunay1 used to run a real test with large amount of data, no graphic



